{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT: [28x28x1]           weights: 0\n",
    "CONV5-32: [28x28x32]       weights: (5*5*1+1)*32\n",
    "POOL2: [14x14x32]          weights: 0\n",
    "CONV5-64: [14x14x64]       weights: (5*5*32+1)*64\n",
    "POOL2: [7x7x64]          weights: 0\n",
    "FC: [1x1x512]              weights: (7*7*64+1)*512\n",
    "FC: [1x1x10]              weights: (1*1*512+1)*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义神经网络相关的参数\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "LAYER1_NODE = 500\n",
    "\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "NUM_LABELS = 10\n",
    "\n",
    "# 第一层卷积层的尺寸和深度\n",
    "CONV1_DEEP = 32\n",
    "CONV1_SIZE = 5\n",
    "# 第二层卷积层的尺寸和深度\n",
    "CONV2_DEEP = 64\n",
    "CONV2_SIZE = 5\n",
    "# 全连接层的节点个数\n",
    "FC_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetModel():\n",
    "    def __init__(self, flags, is_training=False):\n",
    "        self.flags = flags\n",
    "        self.is_training = is_training\n",
    "        self.build_model()\n",
    "        self.build_optimizer()\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.input_x = tf.placeholder(tf.float32, [None, 28, 28, 3], name='input_x')\n",
    "        self.target_y = tf.placeholder(tf.float32, [None, 10], name='target_y')\n",
    "\n",
    "        with tf.variable_scope('layer1-conv1'):\n",
    "            conv1_weights = tf.get_variable('weight', \n",
    "                                           [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP], \n",
    "                                           initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            conv1_biases = tf.get_variable('bias',\n",
    "                                          [CONV1_DEEP], initializer=tf.constant_initializer(0.0))\n",
    "            conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1,1,1,1], padding='SAME')\n",
    "            relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "\n",
    "        with tf.variable_scope('layer2-pool1'):\n",
    "            pool1 = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "        with tf.variable_scope('layer3-conv2'):\n",
    "            conv2_weights = tf.get_variable('weight',\n",
    "                                           [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n",
    "                                           initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            conv2_baises = tf.get_variable('bias',\n",
    "                                          [CONV2_DEEP], initializer=tf.zeros_initializer(0.0))\n",
    "            conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1,1,1,1], padding='SAME')\n",
    "            relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_baises))\n",
    "\n",
    "        with tf.variable_scope('layer4-pool2'):\n",
    "            pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "        pool_shape = pool2.get_shape().as_list()\n",
    "\n",
    "        batch_temp = pool_shape[0]\n",
    "        flatten_size = pool_shape[1]*pool_shape[2]*pool_shape[3]\n",
    "        flatting = tf.reshape(pool2, [batch_temp, flatten_size])\n",
    "\n",
    "        with tf.variable_scope('layer5-fc1'):\n",
    "            fc1_weights = tf.get_variable('weight',\n",
    "                                         [flatten_size, FC_SIZE],\n",
    "                                         initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            fc1_biases = tf.get_variable('bias',\n",
    "                                        [FC_SIZE],\n",
    "                                        initializer=tf.constant_initializer(0.0))\n",
    "            fc1 = tf.nn.relu(tf.matmul(flatting, fc1_weights) + fc1_biases)\n",
    "            if self.is_training:\n",
    "                fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "\n",
    "        with tf.variable_scope('layer6-fc2'):\n",
    "            fc2_weights = tf.get_variable(\"weight\", [FC_SIZE, NUM_LABELS],\n",
    "                                          initializer = tf.truncated_normal_initializer(stddev = 0.1))\n",
    "            fc2_biases = tf.get_variable('bias', [NUM_LABELS], initializer = tf.constant_initializer(0.1))\n",
    "            self.logits = tf.matmul(fc1, fc2_weights) + fc2_biases\n",
    "            \n",
    "    def build_optimizer(self):\n",
    "        self.loss = tf.redcue_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=self.logits, labels=self.target_y))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "            tf.argmax(self.logits, axis=1), \n",
    "            tf.argmax(self.labels, axis=1)), tf.float32))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(self.loss)\n",
    "        \n",
    "    def train(self, data):\n",
    "        saver = tf.train.Saver()\n",
    "        global_step = 0\n",
    "        each_step = 2000\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # writer = tf.summary.FileWriter(self.flags.tensorboard_log_path, sess.graph)\n",
    "            for epoch in range(self.flags.num_epochs):\n",
    "                for _ in range(each_step):\n",
    "                    global_step += 1\n",
    "                    batch_x, batch_y = mnist.train.next_batch(self.flags.batch_size)\n",
    "                    feed_dict = {self.input_x:batch_x, self.target_y:batch_y}\n",
    "                    accu, loss, _ = sess.run([self.accuracy, self.loss, self.optimizer],\n",
    "                                            feed_dict)\n",
    "                    if global_step % 500 == 0:\n",
    "                        print('Epoch: {}; Global Step: {}; accuracy: {:.2f}; loss: {:.2f}'.\n",
    "                              format(epoch + 1, global_step, accu, loss))\n",
    "                saver.save(sess, self.flags.model_save_path)\n",
    "                    \n",
    "    def eval(self):\n",
    "        new_saver = tf.train.Saver()\n",
    "        #checkpath = tf.train.latest_checkpoint()\n",
    "        #print(checkpath)\n",
    "        with tf.Session() as new_sess:\n",
    "            #获取参数到new_sess 中\n",
    "            graph = tf.get_default_graph()\n",
    "            new_saver.restore(new_sess, self.FLAGS.model_save_path)\n",
    "            input_x = graph.get_tensor_by_name('input_x:0')\n",
    "            output_y = graph.get_tensor_by_name('target_y:0')\n",
    "            accuracy = graph.get_tensor_by_name('accuracy:0')\n",
    "            feed = {input_x:mnist.test.images, output_y:mnist.test.labels}\n",
    "            accu = new_sess.run(accuracy, feed_dict=feed)\n",
    "            print('test accuarcy:{:.4f}%'.format(accu*100))        \n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('model_save_path', 'trained_model/dnn_model.ckpt', 'model_save_path')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.01, 'learning_rate')\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, 'number of batch size')\n",
    "tf.app.flags.DEFINE_integer('num_epochs', 1, 'number of epoch to train')\n",
    "tf.app.flags.DEFINE_string('tensorboard_log_path', 'tensorboard_log/', 'tensorboard_log_path')\n",
    "\n",
    "\n",
    "lenet_model = LeNetModel(FLAGS, is_training=True)\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
