# k 近邻算法
## 概述
k近邻算法采用测量不同特征值之间的距离方法进行分类
1. 优点：精度高，对异常值不敏感，无数据输入假定
2. 缺点：计算复杂度高，空间复杂度高
3. 适用数据范围：数值型和标签型

## 算法思路
存在训练样本集，每个数据都存在标签。输入没有标签的数据后，我们选取距离（多种距离计算方法）最近的k个实例，最后，选择k个最相似数据中出现频度最高的分类，作为新数据的分类。

## 算法步骤
1. 收集数据
2. 准备数据：距离计算所需要的数值，最好是结构化的数据格式
3. 分析数据
4. 训练算法：此步骤不适用k近邻算法
5. 测试算法：计算错误率
6. 使用算法


# 决策树
## 概述
1. 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
2. 缺点：可能会产生过度匹配问题
3. 适用数据类型：数值型和标称型



# 朴素贝叶斯
## 概述
1. 优点：在数据较少的情况下仍然有效，可以处理多类别问题
2. 缺点：对于输入数据的准备方式较为敏感
3. 适用数据类型：标称型数据

## tips
1. 为防止每个特征的概率相乘时存在0相导致最后概率等于0，使用拉普拉斯平滑
2. 因为计算概率时，是多个小于零的值相乘，所以对每个概率都取对数然后相加，最后在用指数的方式转换（因为是比较大小，不转换回来也可以） log(a * b) = log(a) + log(b)
